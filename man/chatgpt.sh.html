<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="mountaineerbr" />
  <title>CHATGPT.SH(1) v0.132 | General Commands Manual</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">CHATGPT.SH(1) v0.132 | General Commands Manual</h1>
<p class="author">mountaineerbr</p>
<p class="date">January 2026</p>
</header>
<h1 id="name">NAME</h1>
<div class="line-block">   chatgpt.sh -- Wrapper for ChatGPT / STT /
TTS</div>
<h1 id="synopsis">SYNOPSIS</h1>
<div class="line-block">   <strong>chatgpt.sh</strong>
[<code>-bb</code>|<code>-c</code>|<code>-cd</code>|<code>-dd</code>|<code>-qq</code>]
[<code>opt</code>..]
[<em>PROMPT</em>|<em>TEXT_FILE</em>|<em>PDF_FILE</em>]<br />
   <strong>chatgpt.sh</strong> <code>-w</code> [<code>opt</code>..]
[<em>AUDIO_FILE</em>|<em>.</em>] [<em>LANG</em>] [<em>PROMPT</em>]<br />
   <strong>chatgpt.sh</strong> <code>-W</code> [<code>opt</code>..]
[<em>AUDIO_FILE</em>|<em>.</em>] [<em>PROMPT-EN</em>]<br />
   <strong>chatgpt.sh</strong> <code>-z</code> [<code>opt</code>..]
[<em>OUTFILE</em>|<em>FORMAT</em>|<em>-</em>] [<em>VOICE</em>]
[<em>SPEED</em>] [<em>PROMPT</em>]<br />
   <strong>chatgpt.sh</strong> <code>-bcdWwz</code> [<code>opt</code>..]
-- [<em>PROMPT</em>] -- [<em>stt_arg</em>..] --
[<em>tts_arg</em>..]</div>
<div class="line-block">   <strong>chatgpt.sh</strong> <code>-l</code>
[<em>MODEL</em>]<br />
   <strong>chatgpt.sh</strong> <code>-TTT</code> [-v]
[<code>-m</code>[<em>MODEL</em>|<em>ENCODING</em>]]
[<em>INPUT</em>|<em>TEXT_FILE</em>|<em>PDF_FILE</em>]<br />
   <strong>chatgpt.sh</strong> <code>-HPP</code>
[<code>/</code><em>HIST_NAME</em>|<em>.</em>]<br />
   <strong>chatgpt.sh</strong> <code>-HPw</code></div>
<!--
|    **chatgpt.sh** `-i` \[`opt`..] \[_S_|_M_|_L_]\[_hd_] \[_PROMPT_]  #dall-e-3
|    **chatgpt.sh** `-i` \[`opt`..] \[_X_|_L_|_P_]\[_high_|_medium_|_low_] \[_PROMPT_]  #gpt-image
|    **chatgpt.sh** `-i` \[`opt`..] \[_X_|_L_|_P_]\[_high_|_medium_|_low_] \[_PNG_FILE_]
|    **chatgpt.sh** `-i` \[`opt`..] \[_X_|_L_|_P_]\[_high_|_medium_|_low_] \[_PNG_FILE_] \[_MASK_FILE_] \[_PROMPT_]
-->
<h1 id="description">DESCRIPTION</h1>
<p>Wraps ChatGPT, STT, and TTS endpoints from various providers.</p>
<p>Defaults to single-turn native chat completions. Handles multi-turn
chat, text completions, speech-to-text, and text-to-speech models.</p>
<p>Speech-to-text (STT, Whisper) and text-to-speech (TTS) endpoints are
available to use as stand-alone functions or set to work with multi-turn
chat modes.</p>
<p>Positional arguments are read as a single text PROMPT. Text files and
stdin are appended to the prompt from the command line.</p>
<p>Users can craft model instructions and reuse them, as well as manage
and print out chat sessions.</p>
<h1 id="options">OPTIONS</h1>
<h2 id="interface-modes">Interface Modes</h2>
<dl>
<dt><strong>-b</strong>, <strong>--responses</strong></dt>
<dd>
<p>Responses API calls (may be used with <code>option -c</code> or as
<code>-bb</code>). Set a valid model with “<strong>--model</strong>
[<em>name</em>]”.</p>
</dd>
<dt><strong>-c</strong>, <strong>-cc</strong>,
<strong>--chat</strong></dt>
<dd>
<p>Chat mode in text completions (used with
<code>options -wzvv</code>).</p>
</dd>
<dt><strong>-cd</strong>, <strong>--text-chat</strong></dt>
<dd>
<p>Chat mode in chat completions (used with
<code>options -wzvv</code>).</p>
</dd>
<dt><strong>-C</strong>, <strong>--continue</strong>,
<strong>--resume</strong></dt>
<dd>
<p>Continue from (resume) last session (cmpls/chat).</p>
</dd>
<dt><strong>-d</strong>, <strong>--text</strong></dt>
<dd>
<p>Single-turn session of plain text completions.</p>
</dd>
<dt><strong>-dd</strong></dt>
<dd>
<p>Multi-turn session of plain text completions with history
support.</p>
</dd>
<dt><strong>-e</strong>, <strong>--edit</strong></dt>
<dd>
<p>Edit first input from stdin or file (cmpls/chat).</p>
<p>With <code>options -eex</code>, edit last text editor buffer from
cache.</p>
</dd>
<dt><strong>-E</strong>, <strong>-EE</strong>,
<strong>--exit</strong></dt>
<dd>
<p>Exit on first run (even with options -bcd).</p>
</dd>
<dt><strong>-g</strong>, <strong>--stream</strong>
(<em>defaults</em>)</dt>
<dd>
<p>Response streaming.</p>
</dd>
<dt><strong>-G</strong>, <strong>--no-stream</strong></dt>
<dd>
<p>Unset response streaming.</p>
</dd>
</dl>
<!--
**-i**, **\--image**   \[_PROMPT_]

: Generate images given a prompt.
  Set _option -v_ to not open response.


**-i**   \[_PNG_]

: Create variations of a given image.


**-i**   \[_PNG_] \[_MASK_] \[_PROMPT_]

: Edit image with mask and prompt (required).
-->
<dl>
<dt><strong>-q</strong>, <strong>-qq</strong>,
<strong>--insert</strong></dt>
<dd>
<p>Insert text rather than completing only. May be set twice for
multi-turn.</p>
<p>Use “<em>[insert]</em>” to indicate where the language model should
insert text (`instruct’ and Mistral `code models’).</p>
</dd>
</dl>
<p><strong>-S</strong> <strong>.</strong>[<em>PROMPT_NAME</em>],
<strong>-.</strong>[<em>PROMPT_NAME</em>]</p>
<dl>
<dt><strong>-S</strong> <strong>,</strong>[<em>PROMPT_NAME</em>],
<strong>-,</strong>[<em>PROMPT_NAME</em>]</dt>
<dd>
<p>Load, search for, or create custom prompt.</p>
<p>Set <code>.</code>[<em>PROMPT</em>] to load prompt silently.</p>
<p>Set <code>,</code>[<em>PROMPT</em>] to single-shot edit prompt.</p>
<p>Set <code>,,</code>[<em>PROMPT</em>] to edit the prompt template
file.</p>
<p>Set <code>.</code><em>?</em>, or <code>.</code><em>list</em> to list
all prompt files.</p>
</dd>
</dl>
<p><strong>-S</strong>, <strong>–awesome</strong>
<strong>/</strong>[<em>AWESOME_PROMPT_NAME</em>]</p>
<dl>
<dt><strong>-S</strong>, <strong>–awesome-zh</strong>
<strong>%</strong>[<em>AWESOME_PROMPT_NAME_ZH</em>]</dt>
<dd>
<p>Set or search for an
<strong>awesome-chatgpt-prompt(-zh)</strong>.</p>
<p>Set <strong>//</strong> or <strong>%%</strong> instead to refresh
cache.</p>
</dd>
</dl>
<p><strong>-T</strong>, <strong>--tiktoken</strong></p>
<dl>
<dt><strong>-TT</strong>, <strong>-TTT</strong></dt>
<dd>
<p>Count input tokens with python Tiktoken (ignores special tokens).</p>
<p>Set twice to print tokens, thrice to available encodings.</p>
<p>Set the model or encoding with <code>option -m</code>.</p>
<p>It heeds <code>options -bcdm</code>.</p>
</dd>
<dt><strong>-w</strong>, <strong>--transcribe</strong> [<em>AUD</em>]
[<em>LANG</em>] [<em>PROMPT</em>]</dt>
<dd>
<p>Transcribe audio file speech into text. LANG is optional. A prompt
that matches the speech language is optional. Speech will be transcribed
or translated to the target LANG.</p>
<p>Set twice to phrase or thrice for word-level timestamps (-www).</p>
<p>With <code>options -vv</code>, stop voice recorder on silence auto
detection.</p>
</dd>
<dt><strong>-W</strong>, <strong>--translate</strong> [<em>AUD</em>]
[<em>PROMPT-EN</em>]</dt>
<dd>
<p>Translate audio file speech into English text.</p>
<p>Set twice to phrase or thrice for word-level timestamps (-WWW).</p>
</dd>
<dt><strong>-z</strong>, <strong>--tts</strong>
[<em>OUTFILE</em>|<em>FORMAT</em>|<em>-</em>] [<em>VOICE</em>]
[<em>SPEED</em>] [<em>PROMPT</em>]</dt>
<dd>
<p>Synthesise speech from text prompt. Takes a voice name, speed and
text prompt.</p>
<p>Set <code>option -v</code> to not play response automatically.</p>
</dd>
</dl>
<h2 id="input-modes">Input Modes</h2>
<dl>
<dt><strong>-u</strong>, <strong>--multiline</strong></dt>
<dd>
<p>Toggle multiline prompter, &lt;<em>CTRL-D</em>&gt; flush.</p>
</dd>
<dt><strong>-U</strong>, <strong>--cat</strong></dt>
<dd>
<p>Cat prompter, &lt;<em>CTRL-D</em>&gt; flush.</p>
</dd>
<dt><strong>-x</strong>, <strong>-xx</strong>,
<strong>--editor</strong></dt>
<dd>
<p>Edit prompt in text editor.</p>
<p>Set twice to run the text editor interface a single time for the
first user input.</p>
<p>Set <code>options -eex</code> to edit last buffer from cache.</p>
</dd>
</dl>
<h2 id="model-settings">Model Settings</h2>
<!--
**-\@**, **\--alpha**   \[\[_VAL%_]_COLOUR_]

:      Transparent colour of image mask. Def=_black_.

       Fuzz intensity can be set with \[_VAL%_]. Def=_0%_.
-->
<dl>
<dt><strong>-Nill</strong></dt>
<dd>
<p>Unset model max response tokens (chat cmpls only).</p>
</dd>
</dl>
<p><strong>-NUM</strong></p>
<dl>
<dt><strong>-M</strong>, <strong>--max</strong>
[<em>NUM</em>[<em>-NUM</em>]]</dt>
<dd>
<p>Maximum number of <em>response tokens</em>. Def=<em>4096</em>.</p>
<p>A second number in the argument sets model capacity.</p>
</dd>
<dt><strong>-N</strong>, <strong>--modmax</strong> [<em>NUM</em>]</dt>
<dd>
<p><em>Model capacity</em> token value. Def=<em>auto</em>,
Fallback=<em>8000</em>.</p>
</dd>
<dt><strong>-a</strong>, <strong>--presence-penalty</strong>
[<em>VAL</em>]</dt>
<dd>
<p>Presence penalty (cmpls/chat, -2.0 - 2.0).</p>
</dd>
<dt><strong>-A</strong>, <strong>--frequency-penalty</strong>
[<em>VAL</em>]</dt>
<dd>
<p>Frequency penalty (cmpls/chat, -2.0 - 2.0).</p>
</dd>
</dl>
<!--
**\--best-of**   \[_NUM_]

: Best of results, must be greater than `option -n` (cmpls). Def=_1_.
-->
<p><strong>--effort</strong>
[<em>xhigh</em>|<em>high</em>|<em>medium</em>|<em>low</em>|<em>minimal</em>|<em>none</em>]
(OpenAI)</p>
<dl>
<dt><strong>--think</strong> [<em>token_num</em>] (Anthropic /
Google)</dt>
<dd>
<p>Amount of effort in reasoning models.</p>
</dd>
<dt><strong>--format</strong>
[<em>mp3</em>|<em>wav</em>|<em>flac</em>|<em>opus</em>|<em>aac</em>|<em>pcm16</em>|<em>mulaw</em>|<em>ogg</em>]</dt>
<dd>
<p>TTS out-file format. Def= <em>mp3</em>.</p>
</dd>
<dt><strong>-j</strong>, <strong>--seed</strong> [<em>NUM</em>]</dt>
<dd>
<p>Seed for deterministic sampling (integer).</p>
</dd>
<dt><strong>-K</strong>, <strong>--top-k</strong> [<em>NUM</em>]</dt>
<dd>
<p>Top_k value (local-ai, ollama, google).</p>
</dd>
<dt><strong>--keep-alive</strong>,
<strong>--ka</strong>=[<em>NUM</em>]</dt>
<dd>
<p>How long the model will stay loaded into memory (Ollama).</p>
</dd>
<dt><strong>-m</strong>, <strong>--model</strong> [<em>MODEL</em>]</dt>
<dd>
<p>Language <em>MODEL</em> name.
Def=<em>gpt-5.1</em>/<em>gpt-3.5-turbo-instruct</em>.</p>
<p>Set <em>MODEL</em> name as “<em>.</em>” to pick from the list.</p>
</dd>
<dt><strong>--multimodal</strong>, <strong>--vision</strong>,
<strong>--audio</strong></dt>
<dd>
<p>Model multimodal model type.</p>
</dd>
<dt><strong>-n</strong>, <strong>--results</strong> [<em>NUM</em>]</dt>
<dd>
<p>Number of results. Def=<em>1</em>.</p>
</dd>
<dt><strong>-p</strong>, <strong>--top-p</strong> [<em>VAL</em>]</dt>
<dd>
<p>Top_p value, nucleus sampling (cmpls/chat, 0.0 - 1.0).</p>
</dd>
<dt><strong>-r</strong>, <strong>--restart</strong> [<em>SEQ</em>]</dt>
<dd>
<p>Restart sequence string (cmpls).</p>
</dd>
<dt><strong>-R</strong>, <strong>--start</strong> [<em>SEQ</em>]</dt>
<dd>
<p>Start sequence string (cmpls).</p>
</dd>
<dt><strong>-s</strong>, <strong>--stop</strong> [<em>SEQ</em>]</dt>
<dd>
<p>Stop sequences, up to 4. Def="<em>&lt;|endoftext|&gt;</em>".</p>
</dd>
<dt><strong>-S</strong>, <strong>--instruction</strong>
[<em>INSTRUCTION</em>|<em>FILE</em>]</dt>
<dd>
<p>Set an instruction text prompt. It may be a text file.</p>
</dd>
<dt><strong>--time</strong>, <strong>--no-time</strong>,
<strong>--date</strong>, <strong>--no-date</strong></dt>
<dd>
<p>Prepend the current date and time (timestamp) to the instruction
prompt.</p>
</dd>
<dt><strong>-t</strong>, <strong>--temperature</strong>
[<em>VAL</em>]</dt>
<dd>
<p>Temperature value (cmpls/chat/stt), (0.0 - 2.0, stt 0.0 - 1.0).
Def=<em>0</em>.</p>
</dd>
<dt><strong>--no-truncation</strong></dt>
<dd>
<p>Unset context truncation parameter (Responses API).</p>
</dd>
</dl>
<p><strong>--verbosity</strong>, <strong>--verb</strong>
[<em>high</em>|<em>medium</em>|<em>low</em>]</p>
<dl>
<dt><strong>--no-verbosity</strong></dt>
<dd>
<p>Model response verbosity level (OpenAI).</p>
</dd>
<dt><strong>--voice</strong>
[<em>alloy</em>|<em>fable</em>|<em>onyx</em>|<em>nova</em>|<em>shimmer</em>|<em>ash</em>|<em>ballad</em>|<em>coral</em>|<em>sage</em>|<em>verse</em>|<em>daniel</em>|<em>autumn</em>|<em>diana</em>|<em>hannah</em>|<em>austin</em>|<em>troy</em>]</dt>
<dd>
<p>TTS voice name. OpenAI or GroqAI voice names. Def=<em>echo</em>,
<em>daniel</em>.</p>
</dd>
</dl>
<h2 id="session-and-history-files">Session and History Files</h2>
<dl>
<dt><strong>-H</strong>, <strong>--hist</strong>
[<code>/</code><em>HIST_NAME</em>]</dt>
<dd>
<p>Edit history file with text editor or pipe to stdout.</p>
<p>A history file name can be optionally set as argument.</p>
</dd>
<dt><strong>-P</strong>, <strong>-PP</strong>, <strong>--print</strong>
[<code>/</code><em>HIST_NAME</em>]</dt>
<dd>
<p>Print out last history session.</p>
<p>Set twice to print commented out history entries, inclusive. Heeds
<code>options -bcdrR</code>.</p>
<p>These are aliases to <strong>-HH</strong> and <strong>-HHH</strong>,
respectively.</p>
</dd>
<dt><strong>--tmp</strong></dt>
<dd>
<p>Temporary cache location. Defaults to subdirectory in
<code>$CACHEDIR</code>, <code>$TMPDIR</code>, or <code>/tmp</code>.</p>
</dd>
</dl>
<h2 id="configuration-file">Configuration File</h2>
<dl>
<dt><strong>-f</strong>, <strong>--no-conf</strong></dt>
<dd>
<p>Ignore user configuration file.</p>
</dd>
<dt><strong>-F</strong></dt>
<dd>
<p>Edit configuration file with text editor, if it exists.</p>
<p>$CHATGPTRC="<em>~/.chatgpt.conf</em>".</p>
</dd>
<dt><strong>-FF</strong></dt>
<dd>
<p>Dump template configuration file to stdout.</p>
</dd>
</dl>
<h2 id="service-providers">Service Providers</h2>
<dl>
<dt><strong>--anthropic</strong>, <strong>--ant</strong></dt>
<dd>
<p>Anthropic integration (cmpls/chat). Also see
<strong>--think</strong>.</p>
</dd>
<dt><strong>--deepseek</strong>, <strong>--deep</strong></dt>
<dd>
<p>DeepSeek integration (cmpls/chat).</p>
</dd>
<dt><strong>--github</strong>, <strong>--git</strong></dt>
<dd>
<p>GitHub Models integration (chat).</p>
</dd>
<dt><strong>--google</strong>, <strong>-goo</strong></dt>
<dd>
<p>Google Gemini integration (cmpls/chat).</p>
</dd>
<dt><strong>--groq</strong></dt>
<dd>
<p>Groq AI integration (chat).</p>
</dd>
<dt><strong>--localai</strong></dt>
<dd>
<p>LocalAI integration (cmpls/chat).</p>
</dd>
<dt><strong>--mistral</strong></dt>
<dd>
<p>Mistral AI integration (chat).</p>
</dd>
<dt><strong>--openrouter</strong>, <strong>--open</strong></dt>
<dd>
<p>OpenRouter API integration (cmpls/chat).</p>
</dd>
<dt><strong>--openai</strong></dt>
<dd>
<p>Reset service integrations.</p>
</dd>
<dt><strong>-O</strong>, <strong>--ollama</strong></dt>
<dd>
<p>Ollama server integration (cmpls/chat).</p>
</dd>
<dt><strong>--xai</strong>, <strong>--grok</strong></dt>
<dd>
<p>xAI Grok integration (cmpls/chat).</p>
</dd>
</dl>
<h2 id="miscellaneous-settings">Miscellaneous Settings</h2>
<dl>
<dt><strong>--api-key</strong> [<em>KEY</em>]</dt>
<dd>
<p>The API key to use.</p>
</dd>
<dt><strong>--fold</strong> (<em>defaults</em>),
<strong>--no-fold</strong></dt>
<dd>
<p>Set or unset response folding (wrap at white spaces).</p>
</dd>
<dt><strong>-h</strong>, <strong>--help</strong></dt>
<dd>
<p>Print the help page.</p>
</dd>
<dt><strong>--info</strong></dt>
<dd>
<p>Print OpenAI usage status (requires envar
<code>$OPENAI_ADMIN_KEY</code>).</p>
</dd>
<dt><strong>-k</strong>, <strong>--no-colour</strong></dt>
<dd>
<p>Disable colour output. Def=<em>auto</em>.</p>
</dd>
<dt><strong>-l</strong>, <strong>--list-models</strong>
[<em>MODEL</em>]</dt>
<dd>
<p>List models or print details of <em>MODEL</em>.</p>
</dd>
<dt><strong>-L</strong>, <strong>--log</strong> [<em>FILEPATH</em>]</dt>
<dd>
<p>Log file. <em>FILEPATH</em> is required.</p>
</dd>
<dt><strong>--md</strong>, <strong>--markdown</strong>,
<strong>--markdown</strong>=[<em>SOFTWARE</em>]</dt>
<dd>
<p>Enable markdown rendering in response. Software is optional:
<em>bat</em>, <em>pygmentize</em>, <em>glow</em>, <em>mdcat</em>, or
<em>mdless</em>.</p>
</dd>
<dt><strong>--no-md</strong>, <strong>--no-markdown</strong></dt>
<dd>
<p>Disable markdown rendering.</p>
</dd>
<dt><strong>-o</strong>, <strong>--clipboard</strong></dt>
<dd>
<p>Copy response to clipboard.</p>
</dd>
<dt><strong>--source</strong></dt>
<dd>
<p>Source this script functions and shell configuration (debug).</p>
</dd>
<dt><strong>-v</strong>, <strong>-vv</strong> <!-- verbose --></dt>
<dd>
<p>Less interface verbosity.</p>
<p>Sleep after response in voice chat (<code>-vvbcdw</code>).</p>
<p>With <code>options -bcdwv</code>, sleep after response. With
<code>options -bcdwzvv</code>, stop recording voice input on silence
detection and play TTS response right away.</p>
<p>May be set multiple times.</p>
</dd>
<dt><strong>-V</strong></dt>
<dd>
<p>Dump raw JSON request block (debug).</p>
</dd>
<dt><strong>--version</strong></dt>
<dd>
<p>Print script version.</p>
</dd>
<dt><strong>-y</strong>, <strong>--tik</strong></dt>
<dd>
<p>Tiktoken for token count (cmpls/chat, python).</p>
</dd>
<dt><strong>-Y</strong>, <strong>--no-tik</strong>
(<em>defaults</em>)</dt>
<dd>
<p>Unset tiktoken use (cmpls/chat, python).</p>
</dd>
<dt><strong>-Z</strong>, <strong>-ZZ</strong>, <strong>-ZZZ</strong>,
<strong>--last</strong></dt>
<dd>
<p>Print JSON data of the last responses.</p>
</dd>
</dl>
<h1 id="chat-completion-mode">CHAT COMPLETION MODE</h1>
<p>Invoke <code>option -c</code>, <code>--chat</code> to initiate
interactive multi-turn sessions via <strong>native chat
completions</strong> with persistent history. This mode defaults to the
<em>gpt-5.1</em> model.</p>
<p>Models compatible with <strong>pure text completions</strong>
(instruct models), set <code>options -cd</code> or
<code>--text-chat</code> at the command line. If no model is specified,
it defaults to <em>gpt-3.5-turbo-instruct</em>.</p>
<p>In chat mode, certain internal parameters are automatically tuned to
un-lobotomise the bot for better reasoning.</p>
<p>When using different providers, ensure that <code>options -c</code>,
<code>-cd</code>, or <code>-bc</code> are used according to the specific
model’s capabilities; these options target different API endpoints.</p>
<p>Set <code>option -C</code> to <strong>resume</strong> a previous
history session. Set <code>option -E</code> to
<strong>force-exit</strong> after the first response, even when
multi-turn mode is enabled.</p>
<h1 id="responses-api">RESPONSES API</h1>
<p>The Responses API is a superset of the Chat Completions API. It
offers extended functionality but currently has limited support.</p>
<p>Access the responses endpoint via <code>option -b</code>,
<code>--responses</code> for single-turn incantation, or
<code>options -bb</code> or <code>-bc</code> for multi-turn mode.</p>
<p>To hot-swap to this API during an active session, use the internal
command <code>/responses [model]</code>, aliased to <code>/resp</code>
or even <code>-b</code>.</p>
<h1 id="text-completion-mode">TEXT COMPLETION MODE <!-- legacy --></h1>
<p><code>Option -d</code> starts a single-turn session in <strong>plain
text completions</strong>, no history support. This does not set further
options automatically, such as instruction or temperature.</p>
<p>To run the script in text completions in multi-turn mode and history
support, set command line <code>options -dd</code>.</p>
<p>Set text completion models such as
<em>gpt-3.5-turbo-instruct</em>.</p>
<h1 id="insert-mode-fill-in-the-middle">INSERT MODE
(Fill-In-the-Middle)</h1>
<p>Set <code>option -q</code> for <strong>insert mode</strong> in
single-turn and <code>option -qq</code> for multi-turn. The flag
“<em>[insert]</em>” must be present in the middle of the input prompt.
Insert mode works completing between the end of the text preceding the
flag, and ends completion with the succeeding text after the flag.</p>
<p>Insert mode works with `instruct’ and Mistral `code’ models.</p>
<h1 id="instruction-prompts">INSTRUCTION PROMPTS</h1>
<p>The SYSTEM INSTRUCTION prompt may be set with <code>option -S</code>
or via envars <code>$INSTRUCTION</code> and
<code>$INSTRUCTION_CHAT</code>.</p>
<p><code>Option -S</code> sets an INSTRUCTION prompt (the initial
prompt) for text cmpls, and chat cmpls. A text file, PDF, DOC path, or
URL may be supplied as the single argument or as the last element in a
text prompt. Also see <strong>CUSTOM / AWESOME PROMPTS</strong> section
below.</p>
<p>To create and reuse a custom prompt, set the prompt name as a command
line option, such as “<code>-S .[_prompt_name_]</code>” or
“<code>-S ,[_prompt_name_]</code>”.</p>
<p>When the operator is a comma “<em>,</em>”, single-shot editing will
be available after loading the prompt text. Use double “<em>,,</em>” to
actually edit the template file itself!</p>
<p>Note that loading a custom prompt will also change to its
respectively-named history file.</p>
<p>Alternatively, set the first positional argument with the operator
and the prompt name after any command line options, such as
“<code>chatgpt;sh -c .[_prompt_name_]</code>”. This loads the prompt
file unless instruction was set with command line options.</p>
<p>To prepend the current date and time to the instruction prompt, set
command line <code>option --time</code>.</p>
<p>For TTS <em>gpt-4o-tts</em> model type instructions, set command line
option <code>-S "[instruction]"</code> when invoking the script with
<code>option -z</code> only (stand-alone TTS mode). Alternatively, set
envar <code>$INSTRUCTION_SPEECH</code>.</p>
<p>Note that for audio models such as <code>gpt-4o-audio</code>, the
user can control tone and accent of the rendered voice output with a
robust `INSTRUCTION’ as usual.</p>
<h2 id="prompt-engineering-and-design">Prompt Engineering and
Design</h2>
<p>Minimal <strong>INSTRUCTION</strong> to behave like a chatbot is
given with chat <code>options -c</code>, unless otherwise explicitly set
by the user.</p>
<p>On chat mode, if no INSTRUCTION is set, minimal instruction is given,
and some options auto set, such as increasing temp and presence penalty,
in order to un-lobotomise the bot. <!-- With cheap and fast models of
text cmpls, such as Curie, the \`best_of' option may be worth
setting (to 2 or 3). --></p>
<p>Prompt engineering is an art on itself. Study carefully how to craft
the best prompts to get the most out of text, code and chat cmpls
models.</p>
<p>Certain prompts may return empty responses. Maybe the model has
nothing to further complete input or it expects more text. Try trimming
spaces, appending a full stop/ellipsis, resetting temperature, or adding
more text.</p>
<p>Prompts ending with a space character may result in lower quality
output. This is because the API already incorporates trailing spaces in
its dictionary of tokens.</p>
<p>Note that the model’s steering and capabilities require prompt
engineering to even know that it should answer the questions.</p>
<!--
It is also worth trying to sample 3 - 5 times (increasing the number
of responses with \`option -n 3', for example) in order to obtain
a good response.
-->
<!--
For more on prompt design, see:

 - <https://platform.openai.com/docs/guides/completion/prompt-design>
 - <https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md>
 -->
<h1 id="model-and-capacity">MODEL AND CAPACITY</h1>
<p>Set model with “<code>-m</code> [<em>MODEL</em>]”, with
<em>MODEL</em> as its name, or set it as “<em>.</em>” to pick from the
model list.</p>
<p>List models with <code>option -l</code> or run <code>/models</code>
in chat mode.</p>
<p>Set <em>maximum response tokens</em> with <code>option</code>
“<code>-</code><em>NUM</em>” or “<code>-M</code> <em>NUM</em>”. This
defaults to <em>4096</em> tokens and <em>25000</em> for reasoning
models, or disabled when running on chat completions and responses
endpoints.</p>
<p>If a second <em>NUM</em> is given to this option, <em>maximum model
capacity</em> will also be set. The option syntax takes the form of
“<code>-</code><em>NUM/NUM</em>”, and “<code>-M</code>
<em>NUM-NUM</em>”.</p>
<p><em>Model capacity</em> (maximum model tokens) can be set more
intuitively with <code>option</code> “<code>-N</code> <em>NUM</em>”,
otherwise model capacity is set automatically for known models or to
<em>8000</em> tokens as fallback.</p>
<p><code>Option -y</code> sets python tiktoken instead of the default
script hack to preview token count. This option makes token count
preview accurate and fast (we fork tiktoken as a coprocess for fast
token queries). Useful for rebuilding history context independently from
the original model used to generate responses.</p>
<!-- LocalAI only tested with text and chat completion models (vision) -->
<h1 id="speech-to-text-whisper">SPEECH-TO-TEXT (Whisper)</h1>
<p><code>Option -w</code> <strong>transcribes audio speech</strong> from
<em>mp3</em>, <em>mp4</em>, <em>mpeg</em>, <em>mpga</em>, <em>m4a</em>,
<em>wav</em>, <em>webm</em>, <em>flac</em> and <em>ogg</em> files. First
positional argument must be an <em>AUDIO/VOICE</em> file. Optionally,
set a <em>TWO-LETTER</em> input language (<em>ISO-639-1</em>) as the
second argument. A PROMPT may also be set to guide the model’s style, or
continue a previous audio segment. The text prompt should match the
speech language.</p>
<p>Note that <code>option -w</code> can also be set to <strong>translate
speech</strong> input to any text language to the target language.</p>
<p><code>Option -W</code> <strong>translates speech</strong> stream to
<strong>English text</strong>. A PROMPT in English may be set to guide
the model as the second positional argument.</p>
<p>Set these options twice to have phrasal-level timestamps, options -ww
and -WW. Set thrice for word-level timestamps.</p>
<p>Combine <code>options -wW</code> <strong>with</strong>
<code>options -bcd</code> to start <strong>chat with voice
input</strong> (Whisper) support. Additionally, set
<code>option -z</code> to enable <strong>text-to-speech</strong> (TTS)
models and voice out.</p>
<h1 id="text-to-voice-tts">TEXT-TO-VOICE (TTS)</h1>
<p><code>Option -z</code> synthesises voice from text (TTS models). Set
a <em>voice</em> as the first positional parameter (“<em>alloy</em>”,
“<em>echo</em>”, “<em>fable</em>”, “<em>onyx</em>”, “<em>nova</em>”, or
“<em>shimmer</em>”). Set the second positional parameter as the
<em>voice speed</em> (<em>0.25</em> - <em>4.0</em>), and, finally the
<em>output file name</em> or the <em>format</em>, such as
“<em>./new_audio.mp3</em>” (“<em>mp3</em>”, “<em>wav</em>”,
“<em>flac</em>”, “<em>opus</em>”, “<em>aac</em>”, or “<em>pcm16</em>”);
or set “<em>-</em>” for stdout.</p>
<p>Do mind that GroqAI’s Orpheus has the specific output format
“<em>wav</em>”, as well as different voice names such as daniel, autumn,
diana, etc.</p>
<p>Set <code>options -zv</code> to <em>not</em> play received
output.</p>
<h1 id="multimodal-audio-models">MULTIMODAL AUDIO MODELS</h1>
<p>Audio models, such as <em>gpt-4o-audio</em>, deal with audio input
and output directly.</p>
<p>To activate the microphone recording function of the script, set
command line <code>option -w</code>.</p>
<p>Otherwise, the audio model accepts any compatible audio file (such as
<strong>mp3</strong>, <strong>wav</strong>, and <strong>opus</strong>).
These files can be added to be loaded at the very end of the user prompt
or added with chat command “<code>/audio</code>
<em>path/to/file.mp3</em>”.</p>
<p>To activate the audio synthesis output mode of an audio model, make
sure to set command line <code>option -z</code>!</p>
<!--
# IMAGE GENERATIONS AND EDITS (Dall-E)

`Option -i` **generates images** according to text PROMPT. If the first
positional argument is an _IMAGE_ file, then **generate variations** of
it. If the first positional argument is an _IMAGE_ file and the second
a _MASK_ file (with alpha channel and transparency), and a text PROMPT
(required), then **edit the** _IMAGE_ according to _MASK_ and PROMPT.
If _MASK_ is not provided, _IMAGE_ must have transparency.

The **size of output images** may be set as the first positional parameter
in the command line:

    gpt-image: "_1024x1024_" (_L_, _Large_, _Square_), "_1536x1024_" (_X_, _Landscape_), or "_1024x1536_" (_P_, _Portrait_).

    dall-e-3: "_1024x1024_" (_L_, _Large_, _Square_), "_1792x1024_" (_X_, _Landscape_), or "_1024x1792_" (_P_, _Portrait_).

    dall-e-2: "_256x256_" (_Small_), "_512x512_" (_M_, _Medium_), or "_1024x1024_" (_L_, _Large_).


A parameter "_high_", "_medium_", "_low_", or "_auto_" may also be appended
to the size parameter to set image quality with gpt-image, such as
"_Xhigh_" or "_1563x1024high_". Defaults=_1024x1024auto_.

The parameter "_hd_" or "_standard_" may also be set for image quality with dall-e-3.

For dall-e-3, optionally set the generation style as either "_natural_"
or "_vivid_" as one of the first  positional parameters at command line invocation.

Note that the user needs to verify his organisation to use _gpt-image_ models!

See **IMAGES section** below for more information on **inpaint** and **outpaint**.
-->
<h1 id="text-chat-completions">TEXT / CHAT COMPLETIONS</h1>
<h2 id="text-completions">1. Text Completions</h2>
<p>Given a prompt, the model will return one or more predicted
completions. For example, given a partial input, the language model will
try completing it until probable “<code>&lt;|endoftext|&gt;</code>”, or
other stop sequences (stops may be set with
<code>-s "\[stop-seq]"</code>).</p>
<p><strong>Restart</strong> and <strong>start sequences</strong> may be
optionally set. Restart and start sequences are not set automatically if
the chat mode of text completions is not activated with
<code>option -cd</code>.</p>
<p>Readline is set to work with <strong>multiline input</strong> and
pasting from the clipboard. Alternatively, set <code>option -u</code> to
enable pressing &lt;<em>CTRL-D</em>&gt; to flush input! Or set
<code>option -U</code> to set <em>cat command</em> as input
prompter.</p>
<p>Bash bracketed paste is enabled, meaning multiline input may be
pasted or typed, even without setting <code>options -uU</code>
(<em>v25.2+</em>).</p>
<p>Language model <strong>SKILLS</strong> can be activated with specific
prompts, see <a href="https://platform.openai.com/examples"
class="uri">https://platform.openai.com/examples</a>.</p>
<h2 id="interactive-conversations">2. Interactive Conversations</h2>
<h3 id="native-chat-completions">2.1 Native Chat Completions</h3>
<p>Set the <code>option -c</code> to start chat completions mode. More
recent models are also the best option for many non-chat use cases.</p>
<h3 id="text-completions-chat">2.2 Text Completions Chat</h3>
<p>Set <code>option -cd</code> to start chat mode of text completions.
It keeps a history file, and keeps new questions in context. This works
with a variety of models. Set <code>option -E</code> to exit on
response.</p>
<h3 id="q-a-format">2.3 Q &amp; A Format</h3>
<p>The defaults chat format is “<strong>Q &amp; A</strong>”. The
<strong>restart sequence</strong> “<em>\nQ: </em>” and the <strong>start
text</strong> “<em>\nA:</em>” are injected for the chat bot to work well
with text cmpls.</p>
<p>In multi-turn interactions, special prefixes allow prompt
manipulation: * <strong><code>:</code></strong>_PROMPT_ - Prepends text
to the current <strong>user prompt</strong> before sending. *
<strong><code>::</code></strong>_PROMPT_ - Prepends text to the
<strong>system instruction</strong> for the current turn. *
<strong><code>:::</code></strong> - Re-injects the original system
instruction into the request, useful for reinforcing instructions after
long conversations.</p>
<!--
In multi-turn interactions, prompts prefixed with colon "_:_"
are buffered to be prepended to the user prompt (**USER MESSAGE**)
without incurring an API call. Conversely, prompts starting with double colons
"_::_" are prepended to the instruction prompt (**INSTRUCTION / SYSTEM MESSAGE**).
-->
<p>Entering exactly triple colons “<em>:::</em>” reinjects a system
instruction prompt into the current request. This is useful to reinforce
the instruction when the model’s context has been truncated.</p>
<h3 id="voice-input-stt-and-voice-output-tts">2.4 Voice input (STT), and
voice output (TTS)</h3>
<p>The <code>options -bcdwz</code> may be combined to have voice
recording input and synthesised voice output, specially nice with chat
modes. When setting <code>flag -w</code> or <code>flag -z</code>, the
first positional parameters are read as STT or TTS arguments. When
setting both <code>flags -wz</code>, add a double hyphen to set first
STT, and then TTS arguments.</p>
<p>Set chat mode, plus voice-in transcription language code and text
prompt, and the TTS voice-out option argument:</p>
<pre><code>chatgpt.sh -bcwz  en &#39;transcription prompt&#39;  --  nova</code></pre>
<h3 id="vision-and-multimodal-models">2.5 Vision and Multimodal
Models</h3>
<p>To send an <em>image</em> or <em>url</em> to <strong>vision
models</strong>, either set the image with the “<code>!img</code>”
command with one or more <em>filepaths</em> / <em>urls</em>.</p>
<pre><code>chatgpt.sh -c -m gpt-4-vision-preview &#39;!img path/to/image.jpg&#39;</code></pre>
<p>Alternatively, set the <em>image paths</em> / <em>urls</em> at the
end of the text prompt interactively:</p>
<pre><code>chatgpt.sh -c -m gpt-4-vision-preview

[...]
Q: In this first user prompt, what can you see?  https://i.imgur.com/wpXKyRo.jpeg</code></pre>
<p>Make sure file paths containing spaces are backslash-escaped!</p>
<h3 id="text-pdf-doc-and-url-dumps">2.6 Text, PDF, Doc, and URL
Dumps</h3>
<p>The user may add a <em>filepath</em> or <em>URL</em> to the end of
the text prompt in a multi-turn chat environment.</p>
<p>This format is also supported when passed as an argument to
<strong>option -S</strong>.</p>
<p>The file is then read and the text content added to the prompt. This
is a basic text feature that works with any model.</p>
<pre><code>chatgpt.sh -c

[...]
Q: What is this page: https://example.com

Q: Help me study this paper. ~/Downloads/Prigogine\ Perspective\ on\ Nature.pdf</code></pre>
<p>In the second example, the <em>PDF</em> will be dumped as text.</p>
<p>For PDF text dump support, <code>poppler/abiword</code> is required.
For <em>doc</em> and <em>odt</em> files, <code>LibreOffice</code> is
required. See the <strong>Optional Packages</strong> section.</p>
<p>Also note that <em>file paths</em> containing white spaces must be
<strong>backslash-escaped</strong>, or the <em>file path</em> must be
preceded by a pipe `|’ character.</p>
<p>Multiple images and audio files may be added to the request in this
way!</p>
<h1 id="command-list">COMMAND LIST</h1>
<p>While in chat mode, the following commands can be invoked to change
parameters and manage sessions.</p>
<ul>
<li>Commands can start with either “<code>!</code>” or “<code>/</code>”
and are <em>usually equivalent</em>.</li>
<li>Commands with <strong>colon</strong> (<code>:</code>) add their
output to the current prompt buffer.</li>
<li>Commands with <strong>double dagger</strong> <code>‡</code> execute
as as suffix command, see examples below.</li>
</ul>
<h2 id="command-tables">Command Tables</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Misc</th>
<th style="text-align: left;">Commands</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>-S</code></td>
<td style="text-align: left;">[<em>PROMPT</em>]</td>
<td>Set (overwrite) or unset the system instruction.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-S:</code></td>
<td style="text-align: left;"><code>:</code> [<em>PROMPT</em>]</td>
<td>Prepend to current <em>user</em> prompt.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-S::</code></td>
<td style="text-align: left;"><code>::</code> [<em>PROMPT</em>]</td>
<td>Prepend to system instruction.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-S:::</code></td>
<td style="text-align: left;"><code>:::</code></td>
<td>Restore the previously set system instruction.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-S.</code></td>
<td style="text-align: left;"><code>-.</code> [<em>NAME</em>]</td>
<td>Load and edit custom instruction prompt.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-S/</code></td>
<td style="text-align: left;"><code>!awesome</code> [<em>NAME</em>]</td>
<td>Load and edit awesome prompt (english).</td>
</tr>
<tr>
<td style="text-align: left;"><code>-S%</code></td>
<td style="text-align: left;"><code>!awesome-zh</code>
[<em>NAME</em>]</td>
<td>Load and edit awesome prompt (chinese).</td>
</tr>
<tr>
<td style="text-align: left;"><code>-Z</code></td>
<td style="text-align: left;"><code>!last</code></td>
<td>Print last raw JSON or the processed text response.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!#</code></td>
<td style="text-align: left;"><code>!save</code> [<em>PROMPT</em>]</td>
<td>Save current prompt to shell history (readline). <em>‡</em></td>
</tr>
<tr>
<td style="text-align: left;"><code>!</code></td>
<td style="text-align: left;"><code>!r</code>, <code>!regen</code></td>
<td>Regenerate last response.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!</code></td>
<td style="text-align: left;"><code>!rr</code></td>
<td>Regenerate response, edit prompt first.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!g:</code></td>
<td style="text-align: left;"><code>!!g:</code> [<em>PROMPT</em>]</td>
<td>Ground user prompt with web search results. <em>‡</em></td>
</tr>
<tr>
<td style="text-align: left;"><code>!i</code></td>
<td style="text-align: left;"><code>!info</code> [<em>REGEX</em>]</td>
<td>Information on model and session settings.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!i</code></td>
<td style="text-align: left;"><code>!!info</code></td>
<td>Monthly usage stats (OpenAI).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!j</code></td>
<td style="text-align: left;"><code>!jump</code></td>
<td>Jump to request, append start seq primer (text cmpls).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!j</code></td>
<td style="text-align: left;"><code>!!jump</code></td>
<td>Jump to request, no response priming.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!cache</code></td>
<td style="text-align: left;">-</td>
<td>Toggle prompt caching (<strong>Anthropic only</strong>).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!cat</code></td>
<td style="text-align: left;">-</td>
<td>Cat prompter as one-shot, &lt;<em>CTRL-D</em>&gt; flush.
<em>‡</em></td>
</tr>
<tr>
<td style="text-align: left;"><code>!cat</code></td>
<td style="text-align: left;"><code>!cat:</code>
[<em>TXT</em>|<em>URL</em>|<em>PDF</em>]</td>
<td>Cat <em>text</em>, <em>PDF</em> file, or dump <em>URL</em>.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!clot</code></td>
<td style="text-align: left;"><code>!!clot</code></td>
<td>Flood the TTY with patterns, as visual separator.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!dialog</code></td>
<td style="text-align: left;">-</td>
<td>Toggle the “dialog” interface.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!img</code></td>
<td style="text-align: left;"><code>!media</code>
[<em>FILE</em>|<em>URL</em>]</td>
<td>Add image, media, or URL to prompt.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!md</code></td>
<td style="text-align: left;"><code>!markdown</code>
[<em>SOFTW</em>]</td>
<td>Toggle markdown rendering in response.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!md</code></td>
<td style="text-align: left;"><code>!!markdown</code>
[<em>SOFTW</em>]</td>
<td>Render last response in markdown.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!rep</code></td>
<td style="text-align: left;"><code>!replay</code></td>
<td>Replay last TTS audio response.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!res</code></td>
<td style="text-align: left;"><code>!resubmit</code></td>
<td>Resubmit last STT recorded audio in cache.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!p</code></td>
<td style="text-align: left;"><code>!pick</code> [<em>PROPMT</em>]</td>
<td>File picker, appends filepath to user prompt. <em>‡</em></td>
</tr>
<tr>
<td style="text-align: left;"><code>!pdf</code></td>
<td style="text-align: left;"><code>!pdf:</code> [<em>FILE</em>]</td>
<td>Convert PDF and dump text.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!photo</code></td>
<td style="text-align: left;"><code>!!photo</code> [<em>INDEX</em>]</td>
<td>Take a photo, optionally set camera index (Termux). <em>‡</em></td>
</tr>
<tr>
<td style="text-align: left;"><code>!sh</code></td>
<td style="text-align: left;"><code>!shell</code> [<em>CMD</em>]</td>
<td>Run shell <em>command</em> and edit stdout (make request).
<em>‡</em></td>
</tr>
<tr>
<td style="text-align: left;"><code>!sh:</code></td>
<td style="text-align: left;"><code>!shell:</code> [<em>CMD</em>]</td>
<td>Same as <code>!sh</code> and insert stdout into current prompt.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!sh</code></td>
<td style="text-align: left;"><code>!!shell</code> [<em>CMD</em>]</td>
<td>Run interactive shell <em>command</em> and return.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!time</code></td>
<td style="text-align: left;"><code>!date</code></td>
<td>Add timestamp to the start of user prompt. ‡</td>
</tr>
<tr>
<td style="text-align: left;"><code>!url</code></td>
<td style="text-align: left;"><code>!url:</code> [<em>URL</em>]</td>
<td>Dump URL text or YouTube transcript text.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Script</th>
<th style="text-align: left;">Settings and UX</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>!fold</code></td>
<td style="text-align: left;"><code>!wrap</code></td>
<td>Toggle response wrapping.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-F</code></td>
<td style="text-align: left;"><code>!conf</code></td>
<td>Runtime configuration form TUI.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-g</code></td>
<td style="text-align: left;"><code>!stream</code></td>
<td>Toggle response streaming.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-h</code></td>
<td style="text-align: left;"><code>!help</code> [<em>REGEX</em>]</td>
<td>Print help or grep help for regex.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-l</code></td>
<td style="text-align: left;"><code>!models</code> [<em>NAME</em>]</td>
<td>List language models or show model details.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-o</code></td>
<td style="text-align: left;"><code>!clip</code></td>
<td>Copy responses to clipboard.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-u</code></td>
<td style="text-align: left;"><code>!multi</code></td>
<td>Toggle multiline prompter. &lt;<em>CTRL-D</em>&gt; flush.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-uu</code></td>
<td style="text-align: left;"><code>!!multi</code></td>
<td>Multiline, one-shot. &lt;<em>CTRL-D</em>&gt; flush.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-U</code></td>
<td style="text-align: left;"><code>-UU</code></td>
<td>Toggle cat prompter or set one-shot. &lt;<em>CTRL-D</em>&gt;
flush.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-V</code></td>
<td style="text-align: left;"><code>!debug</code></td>
<td>Dump raw request block and confirm.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-v</code></td>
<td style="text-align: left;">-</td>
<td>Toggle interface verbose modes.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-x</code></td>
<td style="text-align: left;"><code>!ed</code></td>
<td>Toggle text editor interface.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-xx</code></td>
<td style="text-align: left;"><code>!!ed</code></td>
<td>Single-shot text editor.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-y</code></td>
<td style="text-align: left;"><code>!tik</code></td>
<td>Toggle python tiktoken use.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!q</code></td>
<td style="text-align: left;"><code>!quit</code></td>
<td>Exit. Bye.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Settings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>!Nill</code></td>
<td style="text-align: left;"><code>-Nill</code></td>
<td>Unset max response tokens (chat cmpls).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!NUM</code></td>
<td style="text-align: left;"><code>-M</code> [<em>NUM</em>]</td>
<td>Maximum response tokens.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!NUM</code></td>
<td style="text-align: left;"><code>-N</code> [<em>NUM</em>]</td>
<td>Model token capacity.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-a</code></td>
<td style="text-align: left;"><code>!pre</code> [<em>VAL</em>]</td>
<td>Presence penalty.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-A</code></td>
<td style="text-align: left;"><code>!freq</code> [<em>VAL</em>]</td>
<td>Frequency penalty.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-b</code></td>
<td style="text-align: left;"><code>!responses</code>
[<em>MOD</em>]</td>
<td>Responses API request (experimental).</td>
</tr>
<tr>
<td style="text-align: left;"><code>-j</code></td>
<td style="text-align: left;"><code>!seed</code> [<em>NUM</em>]</td>
<td>Seed number (integer).</td>
</tr>
<tr>
<td style="text-align: left;"><code>-K</code></td>
<td style="text-align: left;"><code>!topk</code> [<em>NUM</em>]</td>
<td>Top_k.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-m</code></td>
<td style="text-align: left;"><code>!mod</code> [<em>MOD</em>]</td>
<td>Model by name, empty to pick from list.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-n</code></td>
<td style="text-align: left;"><code>!results</code> [<em>NUM</em>]</td>
<td>Number of results.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-p</code></td>
<td style="text-align: left;"><code>!topp</code> [<em>VAL</em>]</td>
<td>Top_p.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-r</code></td>
<td style="text-align: left;"><code>!restart</code> [<em>SEQ</em>]</td>
<td>Restart sequence.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-R</code></td>
<td style="text-align: left;"><code>!start</code> [<em>SEQ</em>]</td>
<td>Start sequence.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-s</code></td>
<td style="text-align: left;"><code>!stop</code> [<em>SEQ</em>]</td>
<td>One stop sequence.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-t</code></td>
<td style="text-align: left;"><code>!temp</code> [<em>VAL</em>]</td>
<td>Temperature.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-w</code></td>
<td style="text-align: left;"><code>!rec</code> [<em>ARGS</em>]</td>
<td>Toggle voice-in STT. Optionally, set arguments.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-z</code></td>
<td style="text-align: left;"><code>!tts</code> [<em>ARGS</em>]</td>
<td>Toggle TTS chat mode (speech out).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!blk</code></td>
<td style="text-align: left;"><code>!block</code> [<em>ARGS</em>]</td>
<td>Set and add custom options to JSON request.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!effort</code></td>
<td style="text-align: left;">- [<em>MODE</em>]</td>
<td>Effort: xhigh, high, medium, low, minimal, or none (OpenAI).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!think</code></td>
<td style="text-align: left;">- [<em>NUM</em>]</td>
<td>Budget: token value (Anthropic).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!ka</code></td>
<td style="text-align: left;"><code>!keep-alive</code>
[<em>NUM</em>]</td>
<td>Set duration of model load in memory (Ollama).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!verb</code></td>
<td style="text-align: left;"><code>!verbosity</code>
[<em>MODE</em>]</td>
<td>Model verbosity level (high, medium, or low).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!vision</code></td>
<td style="text-align: left;"><code>!audio</code>,
<code>!multimodal</code></td>
<td>Toggle multimodality type.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Session</th>
<th style="text-align: left;">Management</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>-C</code></td>
<td style="text-align: left;">-</td>
<td>Continue current history session (see <code>!break</code>).</td>
</tr>
<tr>
<td style="text-align: left;"><code>-H</code></td>
<td style="text-align: left;"><code>!hist</code> [<em>NUM</em>]</td>
<td>Edit history in editor or print the last <em>n</em> history
entries.</td>
</tr>
<tr>
<td style="text-align: left;"><code>-P</code></td>
<td style="text-align: left;"><code>-HH</code>, <code>!print</code></td>
<td>Print session history (see also <code>!last</code>).</td>
</tr>
<tr>
<td style="text-align: left;"><code>-L</code></td>
<td style="text-align: left;"><code>!log</code> [<em>FILEPATH</em>]</td>
<td>Save to log file.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!c</code></td>
<td style="text-align: left;"><code>!copy</code> [<em>SRC_HIST</em>]
[<em>DEST_HIST</em>]</td>
<td>Copy session from source to destination.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!f</code></td>
<td style="text-align: left;"><code>!fork</code>
[<em>DEST_HIST</em>]</td>
<td>Fork current session and continue from destination.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!k</code></td>
<td style="text-align: left;"><code>!kill</code> [<em>NUM</em>]</td>
<td>Comment out <em>n</em> last entries in history file.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!k</code></td>
<td style="text-align: left;"><code>!!kill</code>
[[<em>0</em>]<em>NUM</em>]</td>
<td>Dry-run of command <code>!kill</code>.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!s</code></td>
<td style="text-align: left;"><code>!session</code>
[<em>HIST_NAME</em>]</td>
<td>Change to, search for, or create history file.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!s</code></td>
<td style="text-align: left;"><code>!!session</code>
[<em>HIST_NAME</em>]</td>
<td>Same as <code>!session</code>, break session.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!u</code></td>
<td style="text-align: left;"><code>!unkill</code> [<em>NUM</em>]</td>
<td>Uncomment <em>n</em> last entries in history file.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!!u</code></td>
<td style="text-align: left;"><code>!!unkill</code>
[[<em>0</em>]<em>NUM</em>]</td>
<td>Dry-run of command <code>!unkill</code>.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!br</code></td>
<td style="text-align: left;"><code>!break</code>,
<code>!new</code></td>
<td>Start new session (session break).</td>
</tr>
<tr>
<td style="text-align: left;"><code>!ls</code></td>
<td style="text-align: left;"><code>!list</code>
[<em>GLOB</em>|<em>.</em>|<em>pr</em>|<em>awe</em>]</td>
<td>List history files with “<em>glob</em>” in <em>name</em>; Files:
“<em>.</em>”; Prompts: “<em>pr</em>”; Awesome: “<em>awe</em>”.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!grep</code></td>
<td style="text-align: left;"><code>!sub</code> [<em>REGEX</em>]</td>
<td>Grep sessions and copy session to hist tail.</td>
</tr>
<tr>
<td style="text-align: left;"><code>!tmp</code></td>
<td style="text-align: left;"><code>!!tmp</code></td>
<td>Fork session to a temporary cache.</td>
</tr>
</tbody>
</table>
<!--
    `best`         `!best-of`    \[_NUM_]      Best-of n results.
    -->
<div class="line-block"><em>:</em> Commands with <strong>colons</strong>
add their output to the current prompt buffer.</div>
<div class="line-block"><em>‡</em> <strong>Double daggers</strong>
indicates suffix-commands, invoked at the end of the user prompt and
preceded by space.</div>
<hr />
<p>Examples</p>
<div class="line-block">  “<code>/temp</code> <em>0.7</em>”,
“<code>!mod</code><em>gpt-5</em>”, “<code>-p</code> <em>0.2</em>”</div>
<div class="line-block">  “<code>/session</code> <em>HIST_NAME</em>”,
“[<em>PROMPT</em>] <code>/pick</code>”</div>
<div class="line-block">  “[<em>PROMPT</em>] <code>/sh</code>”,
“<em>Translate this to French</em> <code>/sh</code>”</div>
<p>Some options can be disabled and excluded from the request by setting
a “<em>-1</em>” as argument (bypass with “<em>-1.0</em>”)</p>
<div class="line-block">  “<code>!presence</code> <em>-1</em>”,
“<code>-a</code> <em>-1</em>”, “<code>-t</code><em>-1</em>”</div>
<hr />
<h2 id="response-regeneration">Response Regeneration</h2>
<p>To <strong>regenerate response</strong>, type in the command
“<code>!regen</code>” or a single exclamation mark or forward slash in
the new empty prompt. In order to edit the prompt before the request,
try “<code>!!</code>” (or “<code>//</code>”).</p>
<h2 id="shell-and-file-integration">Shell and File Integration</h2>
<p>The “<code>/pick</code>” command opens a file picker (usually a
command-line file manager). The selected file path will be appended to
the current prompt in editing mode.</p>
<p>The “<code>/sh</code>” and “<code>/pick</code>” commands may be run
when typed at the end of the current prompt, such as “[<em>PROMPT</em>]
<code>/sh</code>”, which opens a new shell instance to execute commands
interactively. Shell command or file dumps are appended to the current
prompt.</p>
<p>Any command prefixed with a single exclamation mark
(<code>!CMD</code>) that does not match a built-in command is executed
by the shell. This is a shortcut for <code>!sh CMD</code>, and its
standard output is appended to the current prompt.</p>
<p>To execute a shell command without appending its output, use the
double exclamation mark form explicitly (<code>!!sh CMD</code>). This
shell execution facility is exclusive to the <code>!</code>
operator.</p>
<h2 id="api-parameter-injection">API Parameter Injection</h2>
<p>Envar <strong>$BLOCK_USR</strong> can be set to raw model options in
JSON syntax, according to each API, to be injected in the request block.
Alternatively, run command “<code>!block</code> [<em>ARGS</em>]” during
chat mode.</p>
<h1 id="session-management">Session Management</h1>
<p>A history file can hold a single session or multiple sessions. When
it holds a single session, the name of the history file and the session
are the same. However, in case the user breaks a session, the last one
(the tail session) of that history file is always loaded when the resume
<code>option -C</code> is set.</p>
<p>The script uses a <em>TSV file</em> to record entries, which is kept
at the script cache directory (“<code>~/.cache/chatgptsh/</code>”). The
<strong>tail session</strong> of the history file can always be read and
resumed.</p>
<p>Run command “<code>/list [glob]</code>” with optional “<em>glob</em>”
to list session / history “<em>tsv</em>” files. When glob is
“<em>.</em>” list all files in the cache directory; when “<em>pr</em>”
list all instruction prompt files; and when “<em>awe</em>” list all
awesome prompts.</p>
<h2 id="changing-session">Changing Session</h2>
<p>A new history file can be created or changed to with command
“<code>/session</code> [<em>HIST_NAME</em>]”, in which
<em>HIST_NAME</em> is the file name or path of a history file.</p>
<p>On invocation, when the first positional argument to the script
follows the syntax “<code>/</code>[<em>HIST_NAME</em>]”, the command
“<code>/session</code>” is assumed (with
<code>options -bcddCPP</code>).</p>
<h2 id="resuming-and-copying-sessions">Resuming and Copying
Sessions</h2>
<p>To continue from an old session type in a dot “<code>.</code>” or
“<code>/.</code>” as the first positional argument from the command line
on invocation.</p>
<p>The above command is a shortcut of “<code>/copy</code>
<em>current</em> <em>current</em>”. In fact, there are multiple commands
to copy and resume from an older session (the dot means <em>current
session</em>): “<code>/copy . .</code>”, “<code>/fork.</code>”,
“<code>/sub</code>”, and “<code>/grep</code> [<em>REGEX</em>]”.</p>
<p>From the command line on invocation, simply type “<code>.</code>” as
the first positional argument.</p>
<p>It is possible to copy sessions of a history file to another file
when a second argument is given to the “<code>/copy</code>” command.</p>
<p>Mind that forking a session will change to the destination history
file and resume from it as opposed to just copying it.</p>
<!--
If "`/copy` _current_" is run, a selector is shown to choose and copy
a session to the tail of the current history file, and resume it.
This is equivalent to running "`/fork`". -->
<!--
Change to a history file with command "`!session` \[_NAME_]",
and then "`!fork`" the older session to the active session. -->
<!--
Or, "`!copy` \[_ORIGN_] \[_DEST_]" the session from a history file to the current
or other history file.

In these cases, a pickup interface should open to let the user choose
the correct session from the history file. -->
<h2 id="history-file-editing">History File Editing</h2>
<p>To edit chat context at run time, the history file may be modified
with the “<code>/hist</code>” command (also good for context
injection).</p>
<p>Delete history entries or comment them out with “#”.</p>
<p>Note that the user must edit the actual json-escaped strings in the
history file, whilst this is a burdensome strategy, it is acceptable for
small modifications of history context in a live session.</p>
<!--
# CODE COMPLETIONS _(discontinued)_

Codex models are discontinued. Use davinci or _gpt-3.5+ models_ for coding tasks.

-- 
Turn comments into code, complete the next line or function in
context, add code comments, and rewrite code for efficiency,
amongst other functions.
--

Start with a comment with instructions, data or code. To create
useful completions it's helpful to think about what information
a programmer would need to perform a task. 
-->
<!--
# TEXT EDITS  _(discontinued)_

--
This endpoint is set with models with **edit** in their name or
`option -e`. Editing works by setting INSTRUCTION on how to modify
a prompt and the prompt proper.

The edits endpoint can be used to change the tone or structure
of text, or make targeted changes like fixing spelling. Edits
work well on empty prompts, thus enabling text generation similar
to the completions endpoint. 

--

Alternatively, use _gpt-4+ models_ and the right instructions.
-->
<h1 id="custom-prompts">CUSTOM PROMPTS</h1>
<p>When the argument to <code>option -S</code> or the first positional
argument starts with a full stop, such as
“<code>-S.</code><em>my_prompt</em>”, load, search for, or create the
<em>that prompt file</em>.</p>
<p>If a comma is prepended to the prompt name, such as
“-S<code>,</code><em>my_prompt</em>”, load the file and edit it one-shot
for the current session. If a double comma is used instead, such as
“<code>-S,,</code><em>my_prompt</em>”, permanently edit the prompt
template file and then load it. Use “<code>-S .?</code>” to list all
available custom prompts.</p>
<p>These operators also set corresponding history files
automatically.</p>
<p>Do backup your important custom prompts! They are usually located at
“<code>~/.cache/chatgptsh/*.pr</code>”.</p>
<h1 id="awesome-prompts">AWESOME PROMPTS</h1>
<p>When the argument to <code>option -S</code> starts with a forward
slash or a percent sign, such as
“<code>-S/</code><em>linux_terminal</em>”, search for an
<strong>awesome-chatgpt-prompt</strong> (English/Fatih KAv) or
<strong>awesome-chatgpt-prompt-zh</strong> (Chinese/PlexPt).</p>
<p>Set “<code>-S //</code>” or “<code>-S %%</code>” as the argument to
refresh the local prompt cache.</p>
<!--
When the argument to `option -S` starts with a backslash or a percent sign,
such as "`-S` `/`_linux_terminal_", search for an **awesome-chatgpt-prompt(-zh)**
(by Fatih KA and PlexPt). Set "`//`" or "`%%`" to refresh local cache.
Use with _davinci_ and _gpt-3.5+_ models.
-->
<!--
# IMAGES / DALL-E

## 1. Image Generations

An image can be created given a text prompt. A text PROMPT
of the desired image(s) is required. The maximum length is 1000
characters.

This script also supports xAI image generation model
with invocation "`chatgpt.sh --xai -i -m grok-2-image-1212 "[prompt]"`".


## 2. Image Variations

Variations of a given _IMAGE_ can be generated. The _IMAGE_ to use as
the basis for the variations must be a valid PNG file, less than
4MB and square.


## 3. Image Edits

To edit an _IMAGE_, a _MASK_ file may be optionally provided. If _MASK_
is not provided, _IMAGE_ must have transparency, which will be used
as the mask. A text prompt is required.

### 3.1 ImageMagick

If **ImageMagick** is available, input _IMAGE_ and _MASK_ will be checked
and processed to fit dimensions and other requirements.

### 3.2 Transparent Colour and Fuzz

A transparent colour must be set with "`-@`\[_COLOUR_]" to create the
mask. Defaults=_black_.

By defaults, the _COLOUR_ must be exact. Use the \`fuzz option' to match
colours that are close to the target colour. This can be set with
"`-@`\[_VALUE%_]" as a percentage of the maximum possible intensity,
for example "`-@`_10%black_".

See also:

 - <https://imagemagick.org/script/color.php>
 - <https://imagemagick.org/script/command-line-options.php#fuzz>

### 3.3 Mask File / Alpha Channel

An alpha channel is generated with **ImageMagick** from any image
with the set transparent colour (defaults to _black_). In this way,
it is easy to make a mask with any black and white image as a
template.

### 3.4 In-Paint and Out-Paint

In-painting is achieved setting an image with a MASK and a prompt.

Out-painting can also be achieved manually with the aid of this
script. Paint a portion of the outer area of an image with _alpha_,
or a defined _transparent_ _colour_ which will be used as the mask, and set the
same _colour_ in the script with `option -@`. Choose the best result amongst
many results to continue the out-painting process step-wise.
-->
<h1 id="stt-voice-in-whisper">STT / VOICE-IN / WHISPER</h1>
<h2 id="transcriptions">Transcriptions</h2>
<p>Transcribes audio file or voice record into the set language. Set a
<em>two-letter</em> <em>ISO-639-1</em> language code (<em>en</em>,
<em>es</em>, <em>ja</em>, or <em>zh</em>) as the positional argument
following the input audio file. A prompt may also be set as last
positional parameter to help guide the model. This prompt should match
the audio language.</p>
<p>If the last positional argument is “.” or “last” exactly, it will
resubmit the last recorded audio input file from cache.</p>
<p>Note that if the audio language is different from the set language
code, output will be on the language code (translation).</p>
<h2 id="translations">Translations</h2>
<p>Translates audio into <strong>English</strong>. An optional text to
guide the model’s style or continue a previous audio segment is optional
as last positional argument. This prompt should be in English.</p>
<p>Setting <strong>temperature</strong> has an effect, the higher the
more random.</p>
<h1 id="provider-integrations">PROVIDER INTEGRATIONS</h1>
<p>For LocalAI integration, run the script with
<code>option --localai</code>, or set environment
<strong>$OPENAI_BASE_URL</strong> with the server Base URL.</p>
<p>For Mistral AI set environment variable
<strong>$MISTRAL_API_KEY</strong>, and run the script with
<code>option --mistral</code> or set <strong>$OPENAI_BASE_URL</strong>
to “https://api.mistral.ai/”. Prefer setting command line
<code>option --mistral</code> for complete integration.
<!-- also see: \$MISTRAL_BASE_URL --></p>
<p>For Ollama, set <code>option -O</code> (<code>--ollama</code>), and
set <strong>$OLLAMA_BASE_URL</strong> if the server URL is different
from the defaults.</p>
<p>Note that model management (downloading and setting up) must follow
the Ollama project guidelines and own methods.</p>
<!--
For Google Gemini, set environment variable **$GOOGLE_API_KEY**, and
run the script with the command line `option --google`.

For Groq, set the environmental variable `$GROQ_API_KEY`.
Run the script with `option --groq`.
Transcription (Whisper) endpoint available.

For Anthropic, set envar `$ANTHROPIC_API_KEY` and run the script
with command line `option --anthropic`.

For GitHub Models, `$GITHUB_TOKEN` and invoke the script
with `option --github`.
-->
<!--
For Novita API integration, set the environment variable `$NOVITA_API_KEY` and
use the `--novita` option (**legacy**).
-->
<!--
Likewise, for xAI Grok, set environment `$XAI_API_KEY` with its API key.

And for DeepSeek API, set environment `$DEEPSEEK_API_KEY` with its API key.

Run the script with `option --xai` and also with `option -cc` (chat completions.).
-->
<p>Likewise, for other supported service providers, use command line
options, or for unknown providers, use environmental variables for
configuiration.</p>
<p>Many service providers can be wrapped by this script.</p>
<p>Basically, set end export <strong>$OPENAI_BASE_URL</strong> to a
target OpenAI-v1-compatible API host URL before executing the
script.</p>
<p>See section on service providers in our repository documentation, and
an example on how to set up <strong>Novita API integration</strong>.</p>
<p>Some service providers and models may also work with pure text
completions, which is turned on with command-line
<code>option -cd</code> instead.</p>
<p>Prompt caching is manually enabled for Anthropic API and models,
<!-- This feature may be eventually enabled for Google Gemini models, -->
see <strong>BUGS section</strong>.</p>
<h1 id="environment">ENVIRONMENT</h1>
<p><strong>BLOCK_USR</strong></p>
<dl>
<dt><strong>BLOCK_USR_TTS</strong></dt>
<dd>
<p>Extra options for the request JSON block (e.g. “<em>"seed": 33,
"dimensions": 1024</em>”).</p>
</dd>
<dt><strong>CACHEDIR</strong></dt>
<dd>
<p>Script cache directory base.</p>
</dd>
<dt><strong>CHATGPTRC</strong></dt>
<dd>
<p>Path to the user <em>configuration file</em>.</p>
<p>Defaults="<em>~/.chatgpt.conf</em>"</p>
</dd>
<dt><strong>FILECHAT</strong></dt>
<dd>
<p>Path to a history / session TSV file (script-formatted).</p>
</dd>
<dt><strong>INSTRUCTION</strong></dt>
<dd>
<p>Initial initial instruction message.</p>
</dd>
<dt><strong>INSTRUCTION_CHAT</strong></dt>
<dd>
<p>Initial initial instruction or system message in chat mode.</p>
</dd>
<dt><strong>INSTRUCTION_SPEECH</strong></dt>
<dd>
<p>TTS transcription model instruction (gpt-4o-tts models).</p>
</dd>
</dl>
<p><strong>LC_ALL</strong></p>
<dl>
<dt><strong>LANG</strong></dt>
<dd>
<p>Default instruction language in chat mode.
<!-- and Whisper language. --></p>
</dd>
</dl>
<p><strong>MOD_CHAT</strong>, <strong>MOD_AUDIO</strong>,
<strong>MOD_SPEECH</strong>,</p>
<p><strong>MOD_LOCALAI</strong>, <strong>MOD_OLLAMA</strong>,
<strong>MOD_MISTRAL</strong>,</p>
<p><strong>MOD_AUDIO_MISTRAL</strong>, <strong>MOD_GOOGLE</strong>,
<strong>MOD_GROQ</strong>,</p>
<p><strong>MOD_AUDIO_GROQ</strong>, <strong>MOD_SPEECH_GROQ</strong>,
<strong>MOD_ANTHROPIC</strong>,</p>
<dl>
<dt><strong>MOD_GITHUB</strong>, <strong>MOD_OPENROUTER</strong>,
<strong>MOD_XAI</strong>, <strong>MOD_DEEPSEEK</strong></dt>
<dd>
<p>Set default model for each endpoint / provider.</p>
</dd>
</dl>
<p><strong>OPENAI_BASE_URL</strong></p>
<dl>
<dt><strong>OPENAI_URL_PATH</strong></dt>
<dd>
<p>Main Base URL setting. Alternatively, provide a <em>URL_PATH</em>
parameter with the full url path to disable endpoint auto selection.</p>
</dd>
<dt><strong>PROVIDER_BASE_URL</strong></dt>
<dd>
<p>Base URLs for each service provider: <em>LOCALAI</em>,
<em>OLLAMA</em>, <em>MISTRAL</em>, <em>GOOGLE</em>, <em>ANTHROPIC</em>,
<em>GROQ</em>, <em>GITHUB</em>, <em>OPENROUTER</em>, <em>XAI</em>, and
<em>DEEPSEEK</em>.</p>
</dd>
</dl>
<p><strong>OPENAI_API_KEY</strong></p>
<p><strong>PROVIDER_API_KEY</strong></p>
<dl>
<dt><strong>GITHUB_TOKEN</strong></dt>
<dd>
<p>Keys for OpenAI, Gemini, Mistral, Groq, Anthropic, GitHub Models,
OpenRouter, xAI, and DeepSeek APIs.</p>
</dd>
<dt><strong>OUTDIR</strong></dt>
<dd>
<p>Output directory for received audio and files.</p>
</dd>
</dl>
<p><strong>RESTART</strong></p>
<dl>
<dt><strong>START</strong></dt>
<dd>
<p>Restart and start sequences. May be set to <em>null</em>.</p>
<p>Restart=“<em>\nQ: </em>” Start="<em>\nA:</em>" (chat mode)</p>
</dd>
</dl>
<p><strong>VISUAL</strong></p>
<dl>
<dt><strong>EDITOR</strong></dt>
<dd>
<p>Text editor for external prompt editing.</p>
<p>Defaults="<em>vim</em>"</p>
</dd>
<dt><strong>CLIP_CMD</strong></dt>
<dd>
<p>Clipboard set command, e.g. “<em>xsel</em> <em>-b</em>”,
“<em>pbcopy</em>”.</p>
</dd>
<dt><strong>PLAY_CMD</strong></dt>
<dd>
<p>Audio player command, e.g. “<em>mpv –no-video –vo=null</em>”.</p>
</dd>
<dt><strong>REC_CMD</strong></dt>
<dd>
<p>Audio recorder command, e.g. “<em>sox -d</em>”.</p>
</dd>
</dl>
<h1 id="web-search">Web Search</h1>
<h2 id="simple-search-dump">Simple Search Dump</h2>
<p>To ground a user prompt with search results, run chat command
“<code>/g [prompt]</code>”.</p>
<p>Default search provider is Google. To select a different search
provider, run “<code>//g [prompt]</code>” and choose amongst
<em>Google</em>, <em>DuckDuckGo</em>, or <em>Brave</em>.</p>
<p>Running “<code>//g [prompt]</code>” will always use the in-house
solution instead of any service provider specific web search tool.</p>
<p>A cli-browser is required, such as <strong>w3m</strong>,
<strong>elinks, </strong>links<strong>, or </strong>lynx**.</p>
<h2 id="openai-web-search">OpenAI Web Search</h2>
<p>Use the in-house solution above, or select models with “search” in
the name, such as “gpt-4o-search-preview”.</p>
<!--
To enable live search in the API, run chat command `/g [prompt]` or
`//g [prompt]` (to use the fallback mechanism) as usual;
or to keep live search enabled for all prompts, set `$BLOCK_USR`
environment variable before running the script such as:

```
export BLOCK_USR='"tools": [{
  "type": "web_search_preview",
  "search_context_size": "medium"
}]'

chatgpt.sh -cc -m gpt-4.1-2025-04-14
```

Check more search parameters at the OpenAI API documentation:
<https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses>.
<https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat>.
-->
<h2 id="xai-live-search">xAI Live Search</h2>
<p>The xAI live search feature has been discontinued server-side.</p>
<p>Use the in-house solution for simple search text dumps to ground the
prompt. See chat command <code>/g [search_ string]</code>.</p>
<!--
```
export BLOCK_USR='"search_parameters": {
  "mode": "auto",
  "max_search_results": 10
}'

chatgpt.sh --xai -cc -m grok-3-latest 
```

Check more search parameters at the xAI API documentation:
<https://docs.x.ai/docs/guides/live-search>.
-->
<h2 id="anthropic-web-search">Anthropic Web Search</h2>
<pre><code>export BLOCK_USR=&#39;&quot;tools&quot;: [{
  &quot;type&quot;: &quot;web_search_20250305&quot;,
  &quot;name&quot;: &quot;web_search&quot;,
  &quot;max_uses&quot;: 5
}]&#39;

chatgpt.sh --ant -c -m claude-opus-4-0</code></pre>
<p>Check more web search parameters at Anthropic API docs: <a
href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking"
class="uri">https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking</a>.</p>
<h2 id="google-search">Google Search</h2>
<pre><code>export BLOCK_CMD=&#39;&quot;tools&quot;: [ { &quot;google_search&quot;: {} } ]&#39;

chatgpt.sh --goo -c -m gemini-2.5-flash-preview-05-20</code></pre>
<p>Check more web search parameters at Google AI API docs: <a
href="https://ai.google.dev/gemini-api/docs/grounding?lang=rest"
class="uri">https://ai.google.dev/gemini-api/docs/grounding?lang=rest</a>.</p>
<h1 id="color-themes">COLOR THEMES</h1>
<p>The colour scheme may be customised. A few themes are available in
the template configuration file.</p>
<p>A small colour library is available for the user conf file to
personalise the theme colours.</p>
<p>The colour palette is composed of <em>$Red</em>, <em>$Green</em>,
<em>$Yellow</em>, <em>$Blue</em>, <em>$Purple</em>, <em>$Cyan</em>,
<em>$White</em>, <em>$Inv</em> (invert), and <em>$Nc</em> (reset)
variables.</p>
<p>Bold variations are defined as <em>$BRed</em>, <em>$BGreen</em>, etc,
and background colours can be set with <em>$On_Yellow</em>,
<em>$On_Blue</em>, etc.</p>
<p>Alternatively, raw escaped color sequences, such as
<em>\u001b[0;35m</em>, and <em>\u001b[1;36m</em> may be set.</p>
<p>Theme colours are named variables from <code>Colour1</code> to about
<code>Colour11</code>, and may be set with colour-named variables or raw
escape sequences (these must not change cursor position).</p>
<h1 id="configuration-and-cache-files">CONFIGURATION AND CACHE
FILES</h1>
<p>User configuration is stored in <strong>~/.chatgpt.conf</strong>. Its
path location can be set with envar <strong>$CHATGPTRC</strong>.</p>
<p>The script cache directory is <strong>~/.cache/chatgptsh/</strong>
and may contain the following file types:</p>
<ul>
<li><strong>Session Records (tsv):</strong> Tab-separated value files
storing session history. The default session record is
<strong>chatgpt.tsv</strong>.</li>
<li><strong>Prompt Files (pr):</strong> Files storing user-defined
custom instructions (initial prompts).</li>
<li><strong>Command History (history_bash):</strong> Bash command-line
input history. This file is trimmed according to the
<strong>$HISTSIZE</strong> setting in the configuration file. While it
improves session recall, a large <strong>history_bash</strong> file can
slow down script startup. It can be safely removed if necessary.</li>
<li><strong>Temporary Buffers:</strong> Various files holding temporary
text and data. These files are safe to remove and are not intended for
backup.</li>
</ul>
<p><strong>Backup Recommendation:</strong> It is strongly recommended to
back up session record files (tsv) and prompt files (pr), as well as the
configuration file (chatgpt.sh) to preserve session history, custom
promptsnd settings.</p>
<h1 id="keybindings">KEYBINDINGS</h1>
<p>Press &lt;<em>CTRL-X</em> <em>CTRL-E</em>&gt; to edit command line in
text editor from readline.</p>
<p>Press &lt;<em>CTRL-J</em>&gt; or &lt;<em>CTRL-V</em>
<em>CTRL-J</em>&gt; for newline in readline.</p>
<p>Press &lt;<em>CTRL-L</em>&gt; to redraw readline buffer (user input)
on screen.</p>
<p>During <em>cURL</em> requests, press &lt;<em>CTRL-C</em>&gt; once to
interrupt the call.</p>
<p>Press &lt;<em>CTRL-\</em>&gt; to exit from the script (send
<em>QUIT</em> signal), or “<em>Q</em>” in user confirmation prompts.</p>
<h1 id="notes">NOTES</h1>
<p>Stdin text is appended to any existing command line PROMPT.</p>
<p>Input sequences “<em>\n</em>” and “<em>\t</em>” are only treated
specially (as escaped new lines and tabs) in restart, start and stop
sequences.</p>
<p>The moderation endpoint can be accessed by setting the model name to
<em>omni-moderation-latest</em> (or
<em>text-moderation-latest</em>).</p>
<p>For complete model and settings information, refer to OpenAI API docs
at <a href="https://platform.openai.com/docs/"
class="uri">https://platform.openai.com/docs/</a>.</p>
<p>See the online man page and <code>chatgpt.sh</code> usage examples
at: <a href="https://gitlab.com/fenixdragao/shellchatgpt"
class="uri">https://gitlab.com/fenixdragao/shellchatgpt</a>.</p>
<!--
Native chat completions is the **default** mode of script `option -c`
since version 127.
To activate the legacy plain text completions chat mode, set `options -cd`.
  -->
<h1 id="required-packages">REQUIRED PACKAGES</h1>
<ul>
<li><code>Bash</code> shell</li>
<li><code>cURL</code> and <code>JQ</code></li>
</ul>
<h1 id="optional-packages">OPTIONAL PACKAGES</h1>
<p>Optional packages for specific features.</p>
<ul>
<li><code>Base64</code> - Image input in vision models</li>
<li><code>Python</code> - Modules tiktoken, markdown, bs4</li>
<li><code>SoX</code>/<code>Arecord</code>/<code>FFmpeg</code> - Record
input (STT, Whisper)</li>
<li><code>mpv</code>/<code>SoX</code>/<code>Vlc</code>/<code>FFplay</code>/<code>afplay</code>
- Play TTS output</li>
<li><code>xdg-open</code>/<code>open</code>/<code>xsel</code>/<code>xclip</code>/<code>pbcopy</code>
- Open files, set clipboard</li>
<li><code>W3M</code>/<code>Lynx</code>/<code>ELinks</code>/<code>Links</code>
- Dump URL text</li>
<li><code>bat</code>/<code>Pygmentize</code>/<code>Glow</code>/<code>mdcat</code>/<code>mdless</code>
- Markdown support</li>
<li><code>termux-api</code>/<code>termux-tools</code>/<code>play-audio</code>
- Termux system</li>
<li><code>poppler</code>/<code>gs</code>/<code>abiword</code>/<code>ebook-convert</code>/<code>LibreOffice</code>
- Dump PDF or Doc as text</li>
<li><code>dialog</code>/<code>kdialog</code>/<code>zenity</code>/<code>osascript</code>/<code>termux-dialog</code>
- File picker</li>
<li><code>yt-dlp</code> - Dump YouTube captions</li>
</ul>
<!-- - `ImageMagick`/`fbida` - Image edits and variations -->
<h1 id="caveats">CAVEATS</h1>
<p>The script objective is to implement some of the features of OpenAI
API version 1. As text is the only universal interface, voice and image
features will only be partially supported, and not all endpoints or
options will be covered.</p>
<p>This project <em>does not support</em> “Function Calling”,
“Structured Outputs”, “Real-Time Conversations”, “Agents/Operators”,
“MCP Servers”, nor “video generation / editing” capabilities.</p>
<p>Support for “Responses API” is limited and experimental at this
point.</p>
<p>Image generations, variations, and editing endpoints was dropped in
December-2005 with script version v123.</p>
<!--
  chatgpt.sh v122.5  Dec-2025
    https://gitlab.com/fenixdragao/shellchatgpt/-/tree/22f7c89b1dc012e16c796e45ac5c0a3aef9e7e3e
     -->
<h1 id="bugs">BUGS</h1>
<p>Prompt caching may render (havoc) seemigly higher token counts
recorded in the local session history database due to cached tokens,
e.g. xAI reasoning models. Service providers may actually inject (though
not bill) certain amounts of instruction-like tokens automatically.</p>
<p>With reasoning model sessions, if chat context size and recorded
token count do not match, try running the script with command line
<code>option -y</code> to invoke the tiktoken function during context
generation. Be aware that using online tiktoken services may introduce
delays in history generation. This is due to how and if the models
respond with reasoning text or not.</p>
<p>Reasoning (thinking) text and response text from certain API services
may not have clear display separation due to JSON processing
constraints.</p>
<p>Bash “read command” may not correctly display input buffers larger
than the TTY screen size during editing. However, input buffers remain
unaffected. Use the text editor interface for big prompt editing.</p>
<p>If readline screws up your current input buffer, try pressing
&lt;<em>CTRL-L</em>&gt; to force it to redisplay and refresh the prompt
properly on screen.</p>
<p>File paths containing spaces may not work correctly in the chat
interface. Make sure to backslash-escape filepaths with white
spaces.</p>
<p>Folding the response at white spaces may not worked correctly if the
user has changed his terminal tabstop setting. Reset it with command
“tabs -8” or “reset” before starting the script, or set one of these in
the script configuration file.</p>
<p>This script deviates from XDG standards; it expects the configuration
file to be located at <code>~/.chatgpt.sh</code>, and utilises a single
cache directory for both ephemeral data and extended data, including
more persistent files like prompt <code>.pr</code> and session history
<code>.tsv</code> files (which users are expected to backup and manage
themselves =).</p>
<p>If folding does not work well at all, try exporting envar
<code>$COLUMNS</code> before script execution.</p>
<p>Bash truncates input on “\000” (null).</p>
<p>Garbage in, garbage out. An idiot savant.</p>
<p>The script logic resembles a bowl of spaghetti code after a cat
fight.</p>
<!-- Changing models in the same session may generate token count errors
because the recorded token count may differ from model encoding to encoding.
Set `option -y` for accurate token counting. -->
<!-- With the exception of Davinci and newer base models, older models were designed
to be run as one-shot. -->
<!--
`Zsh` does not read history file in non-interactive mode.

`Ksh93` mangles multibyte characters when re-editing input prompt
and truncates input longer than 80 chars. Workaround is to move
cursor one char and press the up arrow key.

`Ksh2020` lacks functionality compared to `Ksh83u+`, such as `read`
with history, so avoid it.
-->
<!--
# EXAMPLES -->
</body>
</html>
